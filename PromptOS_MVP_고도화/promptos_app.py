import streamlit as st
from prompt_generator import classify_intent, extract_conditions, generate_prompt, evaluate_prompt_quality
from llm_api.llm_client import call_llm_openrouter

st.set_page_config(page_title="PromptOS MVP", layout="wide")

st.title("ğŸ§  PromptOS - ì›¹ë²„ì „ MVP")

# 1. ì‚¬ìš©ì ì…ë ¥
utterance = st.text_input("ğŸ’¬ Enter your instruction", placeholder="ì˜ˆ: ìê¸°ì†Œê°œì„œ ì‘ì„±í•´ì¤˜")

if utterance:
    # 2. ì˜ë„ ë¶„ì„
    intent = classify_intent(utterance)
    st.write(f"âœ… Detected Intent: `{intent}`")

    # 3. ì¡°ê±´ ì¶”ì¶œ
    conditions = extract_conditions(utterance)
    st.write("âœ… Extracted Conditions:", conditions)

    # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
    final_prompt = generate_prompt(intent, conditions)
    st.markdown("### ğŸŸ¢ Final Prompt")
    st.code(final_prompt)

    # 5. í’ˆì§ˆ í‰ê°€
    st.markdown("### ğŸ“Š Evaluating Prompt Quality...")
    evaluation = evaluate_prompt_quality(utterance, final_prompt, conditions)
    st.markdown("### ğŸ“‹ Prompt Evaluation Result:")
    st.text(evaluation)

    # 6. LLM ìµœì¢… ì‘ë‹µ
    st.markdown("### ğŸ¤– Calling LLM for final response...")

    system_prompt = "You are a helpful assistant that generates high-quality responses based on the user's prompt."
    gpt_response = call_llm_openrouter(system_prompt, final_prompt)
    
    st.markdown("### ğŸ“¬ GPT Response:")
    st.success(gpt_response)
